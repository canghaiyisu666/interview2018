
# 基本概念

## 是什么
Service mesh是一个网络模型，它是位于TCP/IP之上的抽象层。它假定底层的L3/L4网络是真实存在的，并且能够点对点地传递字节。(它还假定了这个网络像运行环境的其他方面一样，是不可靠的； 因此，Service mesh必须能够处理网络故障。) 

Service mesh在某些方面其实是类似于TCP/IP。就像TCP栈抽象出了网络端点之间可靠传递字节的机制一样，Service mesh在服务之间可靠地传递请求的机制也是抽象的。与TCP相同的是，Service mesh并不关心实际的有效负载，也不关心它的编码问题。应用程序有一个高级目标(“从A到B发送一些数据”)， Service mesh的职责，和TCP一样，是在这个数据的发送过程中解决故障并圆满完成数据发送。

但与TCP不同的是，Service mesh具有更高的性能，它的使命超越了“仅仅让它可以工作”， 并且有一个更重要的目标: 它提供了统一的、应用广泛的观点，应用程序在运行操作时具有可视性和可控性。Service mesh的明确目标是将服务通讯从不可见的、隐含的基础设施中抽离出来，在云原生系统中扮演重要的一级成员的角色,从而可对系统进行监控，管理和控制。

总结 Service Mesh 为：

* 专用基础设施层：独立的运行单元。
* 包括数据层和控制层：数据层负责交付应用请求，控制层控制服务如何通讯。
* 轻量级透明代理：实现形式为轻量级网络代理。
* 处理服务间通讯：主要目的是实现复杂网络中服务间通讯。
* 可靠地交付服务请求：提供网络弹性机制，确保可靠交付请求。
* 与服务部署一起，但服务无需感知：尽管跟应用部署在一起，但对应用是透明的


## 能做什么

* 负载均衡：运行环境中微服务实例通常处于动态变化状态，而且经常可能出现个别实例不能正常提供服务、处理能力减弱、卡顿等现象。但由于所有请求对 Service Mesh 来说是可见的，因此可以通过提供高级负载均衡算法来实现更加智能、高效的流量分发，降低延时，提高可靠性。

* 服务发现：以微服务模式运行的应用变更非常频繁，应用实例的频繁增加减少带来的问题是如何精确地发现新增实例以及避免将请求发送给已不存在的实例变得更加复杂。Service Mesh 可以提供简单、统一、平台无关的多种服务发现机制，如基于 DNS，K/V 键值对存储的服务发现机制。

* 熔断：动态的环境中服务实例中断或者不健康导致服务中断可能会经常发生，这就要求应用或者其他工具具有快速监测并从负载均衡池中移除不提供服务实例的能力，这种能力也称熔断，以此使得应用无需消耗更多不必要的资源不断地尝试，而是快速失败或者降级，甚至这样可避免一些潜在的关联性错误。而 Service Mesh 可以很容易实现基于请求和连接级别的熔断机制。

* 动态路由：随着服务提供商以提供高稳定性、高可用性以及高 SLA 服务为主要目标，为了实现所述目标，出现各种应用部署策略尽可能从技术手段达到无服务中断部署，以此避免变更导致服务的中断和稳定性降低，例如：Blue/Green 部署、Canary 部署，但是实现这些高级部署策略通常非常困难。关于应用部署策略，可参考 Etienne Tremel 
的文章，他对各种部署策略做了详细的比较。而如果运维人员可以轻松的将应用流量从 staging 环境到产线环境，一个版本到另外一个版本，更或者从一个数据中心到另外一个数据中心进行动态切换，甚至可以通过一个中心控制层控制多少比例的流量被切换。那么 Service Mesh 提供的动态路由机制和特定的部署策略如 Blue/Green 部署结合起来，实现上述目标更加容易。

* 安全通讯：无论何时，安全在整个公司、业务系统中都有着举足轻重的位置，也是非常难以实现和控制的部分。而微服务环境中，不同的服务实例间通讯变得更加复杂，那么如何保证这些通讯是在安全、授权情况下进行非常重要。通过将安全机制如 TLS 加解密和授权实现在 Service Mesh 上，不仅可以避免在不同应用的重复实现，而且很容易在整个基础设施层更新安全机制，甚至无需对应用做任何操作。

* 多语言支持：由于 Service Mesh 作为独立运行的透明代理，很容易支持多语言。

* 多协议支持：同多语言支持一样，实现多协议支持也非常容易。

* 指标和分布式追踪：Service Mesh 对整个基础设施层的可见性使得它不仅可以暴露单个服务的运行指标，而且可以暴露整个集群的运行指标。

* 重试和最后期限：Service Mesh 的重试功能避免将其嵌入到业务代码，同时最后期限使得应用允许一个请求的最长生命周期，而不是无休止的重试。

## 业界 Service Mesh 产品

* Buoyant 的 linkerd，基于 Twitter 的 Fingle，长期的实际产线运行经验及验证，支持 Kubernetes，DC/OS 容器管理平台，CNCF 官方支持的项目之一。

* Lyft 的 Envoy，7层代理及通信总线，支持7层 HTTP 路由、TLS、gRPC、服务发现以及健康监测等，也是 CNCF 官方支持项目之一。

* IBM、Google、Lyft 支持的 Istio，一个开源的微服务连接、管理平台以及给微服务提供安全管理，支持 Kubernetes、Mesos 等容器管理工具，其底层依赖于 Envoy。

## linkerd
 linkerd是 Buoyant 开发的快速、轻量级、高性能的，每秒以最小的时延及负载处理万级请求，易于水平扩展，经过产线测试及验证的 Service Mesh 工具，其官方定义为：

> linker∙d is a transparent proxy that adds service discovery, routing, failure handling, and visibility to modern software applications.

总结为：
* 为云原生应用提供弹性的 Service Mesh。
* 透明高性能网络代理。
* 提供服务发现机制、动态路由、错误处理机制及应用运行时可视化。

### 主要功能

1. 基于感知时延的负载均衡
    * 由于 linkerd 工作于 RPC 层，它能实时观测到所有 RPC 请求延时、队列里即将要处理请求的数量，因此可基于实时性能数据分发请求，相对于传统启发式负载均衡算法如 LRU、TCP 活动情况等，这种分发机制性能更优，可尽可能降低延时，提高稳定性。
    * 提供多种负载均衡算法如： `Power of Two Choices (P2C)`: `Least Loaded`、`Power of Two Choices`: `Peak EWMA`、`Aperture`: `Least Loaded`、`Heap`: `Least Loaded`以及`Round-Robin`。

2. 运行时动态路由
    * 支持基于请求级别路由，可在 HTTP 请求里嵌入特定包头，linkerd 将匹配该包头信息的请求路由到特定的应用实例。
    * 动态修改路由规则实现流量迁移、Blue/Green 部署、Canary 部署、跨数据中心 failover 等。

3. 熔断机制
    * 快速失败（Fail Fast）：基于连接的熔断器，如果 linkerd 观测到访问某个应用实例时有连接错误，linkerd 将该实例从维护的连接池移除，与此同时，linkerd 在后台不断尝试与移除的实例建立连接，一旦确认已经恢复，linkerd 重新将其加入连接池。
    * 失败计提（Failure Accrual）：基于请求的熔断器，如果 linkerd 观测到访问某个应用实例时，指定数目请求连续失败，linkerd 将该实例标注为死亡状态，不再接受新的请求尝试。而在后台，linkerd 基于设置的退避间隔发送请求以验证实例是否恢复，一旦恢复，便可接受新的请求。

4. 插入式服务发现

    支持各种服务发现机制如：基于文件（File-based）、DNS 以及基于 KV 键值对的 Zookeeper、Consul 及 Kubernetes，可以很方便的接入现有或者新的服务发现工具。

5. 指标及分布式追踪
    * 作为透明代理使 linkerd 第一时间知道从单个到整个集群应用实例的运行时指标。
    * 除此之外，也可以跟各种分布式追踪系统集成，如 Zipkin，无需对应用做任何调整。

除此之外，linkerd 支持任意开发语言和多种协议如 HTTP/1.1、HTTP/2、gRPC、Thrift、Mux。


[参考博客](http://blog.csdn.net/GitChat/article/details/78872404)



